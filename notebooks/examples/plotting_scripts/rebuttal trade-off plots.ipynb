{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda77672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict as OD\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import jsonpickle\n",
    "import jsonpickle.ext.numpy as jsonpickle_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data to use\n",
    "main_folder = '/Users/mixheikk/Documents/git/DP-PVI/pytorch-code-results/mimic_bal_trade_off_plotting_10clients_5seeds_runs/'\n",
    "runs_to_plot = np.linspace(1,18,18,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define params used in the runs\n",
    "all_eps_sigma = np.asarray([(np.inf,np.inf,4.),(0., 0.,34.1849) ])\n",
    "all_q = np.asarray([5e-3,1e-2,5e-2,.1,.5,1.])\n",
    "all_steps = np.asarray([50])\n",
    "all_C = np.asarray([1.,1000.])\n",
    "\n",
    "nondp_C = 100. # C at least this big with dp_sigma=0 considered to be nonDP\n",
    "\n",
    "restrictions = OD()\n",
    "restrictions['dp_sigma'] = [0.]#,34.1849]\n",
    "restrictions['dp_C'] = [1000.]\n",
    "restrictions['n_global_updates'] = [5]\n",
    "restrictions['n_steps'] = None#[100]\n",
    "restrictions['batch_size'] = None#[5]\n",
    "restrictions['sampling_frac_q'] = [1.,1e-1,1e-2,5e-3]\n",
    "restrictions['pseudo_client_q'] = None#[.1]\n",
    "restrictions['learning_rate'] = None#[5e-3]\n",
    "restrictions['damping_factor'] = None#[.4]\n",
    "restrictions['init_var'] = None#[1e-3]\n",
    "restrictions['dp_mode'] = None#['nondp_batches']\n",
    "restrictions['pre_clip_sigma'] = None#[50.]\n",
    "\n",
    "# possible balance settings: (0,0), (.7,-3), (.75,.95)\n",
    "restrictions['data_bal_rho'] = [.0]\n",
    "restrictions['data_bal_kappa'] = [.0]\n",
    "\n",
    "dataset_name = 'mimic3_bal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colors\n",
    "\n",
    "# default color cycle\n",
    "import pylab\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] # note: stasndard color cycler has 10 colors\n",
    "#cm = plt.get_cmap('viridis')\n",
    "#colors = (cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS))\n",
    "\n",
    "#print(colors, len(colors))\n",
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7322cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data and do initial formatting\n",
    "\n",
    "to_plot = OD()\n",
    "\n",
    "all_res = OD()\n",
    "all_res['config'] = OD()\n",
    "all_res['client_train_res'] = OD()\n",
    "all_res['train_res'] = OD()\n",
    "all_res['validation_res'] = OD()\n",
    "\n",
    "all_baselines = OD()\n",
    "all_baselines['config'] = OD()\n",
    "all_baselines['client_train_res'] = OD()\n",
    "all_baselines['train_res'] = OD()\n",
    "all_baselines['validation_res'] = OD()\n",
    "\n",
    "baseline_folders = []\n",
    "baseline_runs_to_plot = []\n",
    "baseline_names = []\n",
    "\n",
    "jsonpickle_numpy.register_handlers()\n",
    "failed_runs = []\n",
    "\n",
    "for i_run in runs_to_plot:\n",
    "\n",
    "    run_id = str(i_run)\n",
    "    print(f'run {run_id}')\n",
    "    filename = main_folder + run_id + '/config.json'\n",
    "    #print(f'trying {filename}')\n",
    "    tmp = read_config(filename, failed_runs)\n",
    "    if i_run in failed_runs:\n",
    "        continue\n",
    "\n",
    "    all_res['config'][run_id] = tmp[0]\n",
    "\n",
    "    # try opening sacred records, if missing open manual bck instead\n",
    "    filename = main_folder + run_id + '/info.json'\n",
    "    filename_bck = main_folder + run_id + '/info_bck.json'\n",
    "    apu = read_results(filename, filename_bck)\n",
    "    \n",
    "    # format results for plotting\n",
    "    client_measures = ['elbo','kl','logl']\n",
    "    format_results(apu, run_id, client_measures, all_res)\n",
    "\n",
    "if len(failed_runs) > 0:\n",
    "    print(f'failed runs:\\n{failed_runs}')\n",
    "    runs_to_plot = list(runs_to_plot)\n",
    "    for i_run in failed_runs:\n",
    "        runs_to_plot.remove(i_run)\n",
    "    runs_to_plot = np.array(runs_to_plot)\n",
    "\n",
    "# read baselines\n",
    "if len(baseline_folders) > 0:\n",
    "    running_id = 0\n",
    "    for folder, baseline_name, baseline_ids in zip(baseline_folders, baseline_names, baseline_runs_to_plot):\n",
    "        for i_run in baseline_ids:\n",
    "            run_id = str(running_id)\n",
    "            print(f'baseline run {run_id}')\n",
    "            filename = folder + str(i_run) + '/config.json'\n",
    "            #print(f'trying {filename}')\n",
    "            tmp = read_config(filename, failed_runs)\n",
    "            all_baselines['config'][run_id] = tmp[0]\n",
    "\n",
    "            # try opening sacred records, if missing open manual bck instead\n",
    "            filename = folder + str(i_run) + '/info.json'\n",
    "            filename_bck = folder + str(i_run) + '/info_bck.json'\n",
    "            apu = read_results(filename, filename_bck)\n",
    "\n",
    "            #print(apu)\n",
    "            #sys.exit()\n",
    "\n",
    "            #for k in apu:\n",
    "            #    print(k)\n",
    "            #sys.exit()\n",
    "            \n",
    "            # format results for plotting\n",
    "            client_measures = ['elbo','kl','logl']\n",
    "            format_results(apu, run_id, client_measures, all_baselines)\n",
    "\n",
    "            running_id += 1\n",
    "\n",
    "# check restrictions\n",
    "list_to_print = []\n",
    "for i_run in runs_to_plot:\n",
    "    print_this = True\n",
    "    for k in restrictions:\n",
    "        try:\n",
    "            if restrictions[k] is not None and all_res['config'][str(i_run)][k] not in restrictions[k]:\n",
    "                print_this = False\n",
    "        except:\n",
    "            continue\n",
    "    # check baselines\n",
    "    '''\n",
    "    if include_baselines and not print_this:\n",
    "        for tmp in baselines:\n",
    "            print_this = True\n",
    "            for k in tmp:\n",
    "                if tmp[k] is not None and all_res['config'][str(i_run)][k] != tmp[k]:\n",
    "                    print_this = False\n",
    "                    break\n",
    "            if print_this:\n",
    "                break\n",
    "    '''\n",
    "    if print_this:\n",
    "        list_to_print.append(i_run)\n",
    "if len(list_to_print) == 0:\n",
    "    sys.exit('No runs satisfying restrictions found!')\n",
    "else:\n",
    "    print(f'Found {len(list_to_print)} runs to plot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_type = 'q_trade_off_rebuttal'\n",
    "plot_type = 'q_trade_off_rebuttal2'\n",
    "\n",
    "if plot_type == 'q_trade_off_rebuttal':\n",
    "    # plot mean (over seeds) acc/logl against q values, with fixed eps, C; use max performance on any global\n",
    "    to_plot['best_mean_acc'] = np.zeros((2,len(all_eps_sigma[0]), len(all_q)))\n",
    "    to_plot['best_mean_logl'] = np.zeros((2,len(all_eps_sigma[0]), len(all_q)))\n",
    "    to_plot['best_mean_avg_prec_score'] = np.zeros((2,len(all_eps_sigma[0]), len(all_q)))\n",
    "    to_plot['mean_ROC_at_best_mean_logl'] = OD()\n",
    "    to_plot['dp_C'] = np.zeros(len(all_eps_sigma[0]))\n",
    "    \n",
    "elif plot_type == 'q_trade_off_rebuttal2':\n",
    "    # plot q as different lines; acc/logl against global update to show convergence speed\n",
    "    # these are now not best means, but just means over seeds\n",
    "    # take number of globals from any run; should be same for all to make any sense\n",
    "    to_plot['mean_acc'] = np.zeros((2,len(restrictions['sampling_frac_q']), all_res['config']['1']['n_global_updates']))\n",
    "    to_plot['mean_logl'] = np.zeros((2,len(restrictions['sampling_frac_q']), all_res['config']['1']['n_global_updates']))\n",
    "    to_plot['mean_avg_prec_score'] = np.zeros((2, len(restrictions['sampling_frac_q']),all_res['config']['1']['n_global_updates']))\n",
    "    to_plot['mean_ROC_at_best_mean_logl'] = OD()\n",
    "    #to_plot['dp_C'] = np.zeros(len(list_to_print))\n",
    "    to_plot['dp_C'] = np.zeros(len(restrictions['sampling_frac_q']))\n",
    "\n",
    "else:\n",
    "    sys.exit(f'Unknown plot type: {plot_type}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee254a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data for plotting\n",
    "\n",
    "for i_line,i_run in enumerate(list_to_print):\n",
    "\n",
    "    config = all_res['config'][str(i_run)]\n",
    "    res = all_res['validation_res'][str(i_run)]\n",
    "    \n",
    "    # best mean over all global updates\n",
    "    tmp = ['acc', 'logl', 'avg_prec_score']\n",
    "    for i_tmp,tmp_name in enumerate(tmp):\n",
    "        # take argmax logl as the best model\n",
    "        #print(all_res['validation_res'][str(i_run)]['logl'].shape)\n",
    "        i_max = np.argmax(all_res['validation_res'][str(i_run)]['logl'],0)\n",
    "        #print(i_max)\n",
    "        #sys.exit()\n",
    "        \n",
    "        if plot_type in ['eps_trade_off']:\n",
    "            raise NotImplementedError('fix argmax')\n",
    "            #logging.warning('check argmax here!')\n",
    "            if config['dp_sigma'] != 0 and config['dp_sigma'] is not None:\n",
    "                i_max = np.argmax(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "                to_plot[f'best_mean_{tmp_name}'][0,all_eps_sigma[1] == config['dp_sigma'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].mean(-1)[i_max]\n",
    "                to_plot[f'best_mean_{tmp_name}'][1,all_eps_sigma[1] == config['dp_sigma'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].std(-1)[i_max]\n",
    "\n",
    "            else:\n",
    "                if config['dp_C'] < nondp_C:\n",
    "                    # only clipping\n",
    "                    i_eps = 1\n",
    "                else:\n",
    "                    # nonDP\n",
    "                    if i_tmp == 0:\n",
    "                        try:\n",
    "                            print(f\"nondp run: {i_line}: dp_C={config['dp_C']}, dp_sigma={config['dp_sigma']}, sampling q={config['sampling_frac_q']}, pseudo q={config['pseudo_client_q']}\")\n",
    "                        except:\n",
    "                            print(\"nondp doesn't have pseudo client conf?\")\n",
    "                    i_eps = 0\n",
    "\n",
    "                i_max = np.argmax(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "                to_plot[f'best_mean_{tmp_name}'][0,i_eps, all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].mean(-1)[i_max]\n",
    "                to_plot[f'best_mean_{tmp_name}'][1,i_eps, all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].std(-1)[i_max]\n",
    "\n",
    "        elif plot_type == 'q_trade_off':\n",
    "            raise NotImplementedError('fix argmax')\n",
    "            #print(config['n_steps'], all_steps, all_steps == config['n_steps'])\n",
    "            i_max = np.argmax(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "            to_plot[f'best_mean_{tmp_name}'][0, all_steps == config['n_steps'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].mean(-1)[i_max]\n",
    "            to_plot[f'best_mean_{tmp_name}'][1,all_steps == config['n_steps'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].std(-1)[i_max]\n",
    "\n",
    "        elif plot_type == 'q_trade_off_with_C':\n",
    "            # NOTE: check that works with several seeds if used\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name][i_max].shape)\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name][i_max].mean(-1))\n",
    "\n",
    "            #print(config['n_steps'], all_steps, all_steps == config['n_steps'])\n",
    "            #i_max = np.argmax(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "            to_plot[f'best_mean_{tmp_name}'][0, all_C == config['dp_C'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name][i_max].mean(-1)\n",
    "            to_plot[f'best_mean_{tmp_name}'][1,all_C == config['dp_C'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name][i_max].std(-1)\n",
    "            #i_max = np.argmax(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "            #to_plot[f'best_mean_{tmp_name}'][0, all_C == config['dp_C'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].mean(-1)[i_max]\n",
    "            #to_plot[f'best_mean_{tmp_name}'][1,all_C == config['dp_C'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].std(-1)[i_max]\n",
    "\n",
    "\n",
    "        elif plot_type in ['q_trade_off_rebuttal']:\n",
    "            # want separate line for each eps and (possibly) dp_C\n",
    "            #print(i_run,tmp_name)\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name].shape)\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name])\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name][i_max].mean(-1))\n",
    "            \n",
    "            if config['dp_sigma'] != 0 and config['dp_sigma'] is not None:\n",
    "                to_plot['dp_C'][all_eps_sigma[1] == config['dp_sigma']] = config['dp_C']\n",
    "                \n",
    "                i_max = np.argmax(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "                to_plot[f'best_mean_{tmp_name}'][0,all_eps_sigma[1] == config['dp_sigma'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].mean(-1)[i_max]\n",
    "                to_plot[f'best_mean_{tmp_name}'][1,all_eps_sigma[1] == config['dp_sigma'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].std(-1)[i_max]\n",
    "            else:\n",
    "                if i_tmp == 0:\n",
    "                    print(f\"nondp run: {i_line}: dp_C={config['dp_C']}, dp_sigma={config['dp_sigma']}, sampling q={config['sampling_frac_q']}\")\n",
    "\n",
    "                if config['dp_C'] < nondp_C:\n",
    "                    # only clipping\n",
    "                    i_eps = 1\n",
    "                else:\n",
    "                    # nonDP\n",
    "                    i_eps = 0\n",
    "                i_max = np.argmax(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "                to_plot[f'best_mean_{tmp_name}'][0,i_eps, all_q == config['sampling_frac_q']] = all_res['validation_res'][str(i_run)][tmp_name].mean(-1)[i_max]\n",
    "                to_plot[f'best_mean_{tmp_name}'][1,i_eps, all_q == config['sampling_frac_q']] = all_res['validation_res'][str(i_run)][tmp_name].std(-1)[i_max]\n",
    "                to_plot['dp_C'][i_eps] = config['dp_C']\n",
    "\n",
    "\n",
    "        elif plot_type in ['q_trade_off_rebuttal2']:\n",
    "            # separate line for each q, eps and C\n",
    "            \n",
    "            #print(i_run,tmp_name)\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name].shape)\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name])\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name][i_max].mean(-1))\n",
    "\n",
    "            # this needs array as well!\n",
    "            to_plot['dp_C'][restrictions['sampling_frac_q'] == config['sampling_frac_q']] = config['dp_C']\n",
    "\n",
    "            \"\"\"\n",
    "            if config['dp_sigma'] != 0 and config['dp_sigma'] is not None:\n",
    "                #i_max = np.argmax(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "                to_plot[f'mean_{tmp_name}'][0, all_eps_sigma[1] == config['dp_sigma'], all_q == config['sampling_frac_q'],:]  = all_res['validation_res'][str(i_run)][tmp_name].mean(-1)\n",
    "                to_plot[f'mean_{tmp_name}'][1, all_eps_sigma[1] == config['dp_sigma'], all_q == config['sampling_frac_q'] ]  = all_res['validation_res'][str(i_run)][tmp_name].std(-1)\n",
    "            else:\n",
    "            \n",
    "            if config['dp_C'] < nondp_C:\n",
    "                # only clipping\n",
    "                i_eps = 1\n",
    "            else:\n",
    "                # nonDP\n",
    "                if i_tmp == 0:\n",
    "                    try:\n",
    "                        print(f\"nondp run: {i_line}: dp_C={config['dp_C']}, dp_sigma={config['dp_sigma']}, sampling q={config['sampling_frac_q']}, pseudo q={config['pseudo_client_q']}\")\n",
    "                    except:\n",
    "                        print(\"nondp doesn't have pseudo client conf?\")\n",
    "                i_eps = 0\n",
    "            \"\"\"\n",
    "            to_plot[f'mean_{tmp_name}'][0, np.asarray(restrictions['sampling_frac_q']) == config['sampling_frac_q'],:]  = all_res['validation_res'][str(i_run)][tmp_name].mean(-1)\n",
    "            to_plot[f'mean_{tmp_name}'][1, np.asarray(restrictions['sampling_frac_q']) == config['sampling_frac_q'],:]  = all_res['validation_res'][str(i_run)][tmp_name].std(-1)\n",
    "            #print(restrictions['sampling_frac_q'],config['sampling_frac_q'], np.asarray(restrictions['sampling_frac_q']) == config['sampling_frac_q'])\n",
    "            #print(all_res['validation_res'][str(i_run)][tmp_name].mean(-1))\n",
    "#to_plot['mean_acc'] = np.zeros((2,len(all_eps_sigma[0]),len(all_q), all_res['config']['1']['n_global_updates']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acc/logl vs q (number of local splits)\n",
    "# separate line for each eps or dp_C\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "#plt.suptitle(f\"Included clipping C: {restrictions['dp_C']}\")\n",
    "for i_line, eps in enumerate(all_eps_sigma[0]):\n",
    "    C = to_plot['dp_C'][i_line]\n",
    "    \n",
    "    axs[0,0].errorbar(np.log10(all_q), to_plot['best_mean_acc'][0,i_line,:], \n",
    "                    yerr= 2*to_plot['best_mean_acc'][1,i_line,:]/np.sqrt(config['n_rng_seeds']), # 2*SEM errorbar over seeds\n",
    "                    label=f'eps={eps}, C={C}', \n",
    "                    color=colors[i_line%len(colors)]\n",
    "                    )\n",
    "    axs[1,0].errorbar(np.log10(all_q), to_plot['best_mean_logl'][0,i_line,:], \n",
    "                    yerr= 2*to_plot['best_mean_logl'][1,i_line,:]/np.sqrt(config['n_rng_seeds']), # 2*SEM errorbar over seeds\n",
    "                    label=f'eps={eps}, C={C}', \n",
    "                    color=colors[i_line%len(colors)]\n",
    "                    )\n",
    "    #axs[0,1].errorbar(np.log10(all_q), to_plot['best_mean_avg_prec_score'][0,i_line,:], \n",
    "                    yerr= 2*to_plot['best_mean_avg_prec_score'][1,i_line,:]/np.sqrt(config['n_rng_seeds']), # 2*SEM errorbar over seeds\n",
    "                    label=f'eps={eps}, C={C}', \n",
    "                    color=colors[i_line%len(colors)]\n",
    "                    )\n",
    "    axs[1,1].plot(0,0, label=f\"eps={eps}, C={C}\") # this is currently just used for labels\n",
    "axs[1,1].tick_params(axis='both',which='both',bottom=False,left=False,labelbottom=False, labelleft=False)\n",
    "#\n",
    "# invert axes when plotting number of local splits\n",
    "#axs[0,0].set_xlim(100,1)\n",
    "axs[1,1].legend()\n",
    "axs[0,0].grid()\n",
    "axs[1,0].grid()\n",
    "axs[0,1].grid()\n",
    "axs[0,0].set_ylabel('Acc')\n",
    "axs[1,0].set_ylabel('Logl')\n",
    "axs[0,1].set_ylabel('Avg prec score')\n",
    "axs[0,0].set_xlabel('log2 fraction of local samples per split')\n",
    "axs[1,0].set_xlabel('log2 fraction of local samples per split')\n",
    "axs[0,1].set_xlabel('log2 fraction of local samples per split')\n",
    "plt.tight_layout()\n",
    "#if to_disk:\n",
    "#    plt.savefig(fig_folder + plot_filename)\n",
    "#else:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eps_sigma[0][all_res['config'][str(list_to_print[0])]['dp_sigma'] == all_eps_sigma[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd06732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acc/logl vs global update\n",
    "# \n",
    "\n",
    "legend_font_size = 14\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(8,10))\n",
    "plt.suptitle(f\"Log. regr. with balanced MIMIC-III, 10 clients, balanced split\", fontsize=legend_font_size+2)\n",
    "#for i_line, eps in enumerate(all_eps_sigma[0]):\n",
    "#for i_line, i_run in enumerate(list_to_print):\n",
    "for i_q, q in enumerate(restrictions['sampling_frac_q']):\n",
    "    \n",
    "    #C = all_res['config'][str(i_run)]['dp_C']\n",
    "    C = to_plot['dp_C'][i_q]\n",
    "    #eps = all_eps_sigma[0][all_res['config'][str(list_to_print[0])]['dp_sigma'] == all_eps_sigma[1]]\n",
    "    #try:\n",
    "    #    eps = eps[0]\n",
    "    #except:\n",
    "    #    pass\n",
    "    \n",
    "    #n_globals = all_res['config'][str(i_run)]['n_global_updates']\n",
    "    #q = all_res['config'][str(i_run)]['sampling_frac_q']\n",
    "    n_globals = 5\n",
    "    \n",
    "    #axs[0,0].errorbar(np.linspace(1,n_globals,n_globals), all_res['validation_res'][str(i_run)]['acc'].mean(-1), \n",
    "        #yerr= 2*all_res['validation_res'][str(i_run)]['acc'].std(-1)/np.sqrt(config['n_rng_seeds']), # 2*SEM errorbar over seeds\n",
    "    axs[0].errorbar(np.linspace(1,n_globals,n_globals, dtype='int'), to_plot['mean_acc'][0,i_q,:], \n",
    "                    yerr= 2*to_plot['mean_acc'][1,i_q,:]/np.sqrt(config['n_rng_seeds']), # 2*SEM errorbar over seeds\n",
    "                    #label=f'q={q},eps={eps}, C={C}', \n",
    "                    label=f'Number of local splits: {int(1/q)}', \n",
    "                    color=colors[i_q%len(colors)], markersize=12, \n",
    "                    )\n",
    "    #axs[1,0].errorbar(np.linspace(1,n_globals,n_globals), all_res['validation_res'][str(i_run)]['logl'].mean(-1), \n",
    "    #                yerr= 2*all_res['validation_res'][str(i_run)]['logl'].std(-1)/np.sqrt(config['n_rng_seeds']), # 2*SEM errorbar over seeds\n",
    "    axs[1].errorbar(np.linspace(1,n_globals,n_globals, dtype='int'), to_plot['mean_logl'][0,i_q,:], \n",
    "                    yerr= 2*to_plot['mean_logl'][1,i_q,:]/np.sqrt(config['n_rng_seeds']), # 2*SEM errorbar over seeds\n",
    "                    #label=f'q={q},eps={eps}, C={C}', \n",
    "                    label=f'Number of local splits: {int(1/q)}', \n",
    "                    color=colors[i_q%len(colors)], markersize=12, \n",
    "                    )\n",
    "\n",
    "# invert axes when plotting number of local splits\n",
    "#axs[0,0].set_xlim(100,1)\n",
    "axs[0].set_ylim((.5,.78))\n",
    "axs[1].set_ylim((-1.,-.45))\n",
    "#axs[1,1].legend()\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "#axs[0,1].grid()\n",
    "axs[0].set_ylabel('Accuracy', fontsize=legend_font_size+1)\n",
    "axs[1].set_ylabel('Logl', fontsize=legend_font_size+1)\n",
    "#axs[0,1].set_ylabel('Avg prec score')\n",
    "axs[0].set_xlabel('Communications', fontsize=legend_font_size+1)#axs[1,0].set_xlabel('Communications')\n",
    "axs[1].set_xlabel('Communications', fontsize=legend_font_size+1)\n",
    "axs[0].legend(loc=4, fontsize=legend_font_size)\n",
    "axs[1].legend(loc=4, fontsize=legend_font_size)\n",
    "#axs[1].legend()\n",
    "plt.tight_layout()\n",
    "#if to_disk:\n",
    "#plt.savefig('figs/' + 'mimic_bal_trade-off_non-DP.pdf')\n",
    "#else:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acc/logl vs q (number of local splits)\n",
    "# separate line for each eps or dp_C\n",
    "legend_font_size = 14\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8,10))\n",
    "plt.suptitle(f\"Log. regr. with balanced MIMIC-III, 10 clients, balanced split\", fontsize=legend_font_size+2)\n",
    "for i_line, eps in enumerate(all_eps_sigma[0]):\n",
    "    C = to_plot['dp_C'][i_line]\n",
    "    \n",
    "    axs[0].errorbar(1/all_q, to_plot['best_mean_acc'][0,i_line,:], \n",
    "                    yerr= 2*to_plot['best_mean_acc'][1,i_line,:]/np.sqrt(config['n_rng_seeds']), # 2*SEM errorbar over seeds\n",
    "                    label=f'eps={eps}, C={int(C)}', \n",
    "                    color=colors[i_line%len(colors)], markersize=12, \n",
    "                    )\n",
    "    axs[1].errorbar(1/all_q, to_plot['best_mean_logl'][0,i_line,:], \n",
    "                    yerr= 2*to_plot['best_mean_logl'][1,i_line,:]/np.sqrt(config['n_rng_seeds']), # 2*SEM errorbar over seeds\n",
    "                    label=f'eps={eps}, C={int(C)}', \n",
    "                    color=colors[i_line%len(colors)], markersize=12, \n",
    "                    )\n",
    "\n",
    "# invert axes when plotting number of local splits\n",
    "#axs[0].set_xlim(200,1)\n",
    "axs[0].legend(loc=4, fontsize=legend_font_size)\n",
    "axs[1].legend(loc=4, fontsize=legend_font_size)\n",
    "axs[0].set_ylim((.5,.78))\n",
    "axs[1].set_ylim((-1.,-.45))\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "axs[0].set_ylabel('Accuracy', fontsize=legend_font_size+1)\n",
    "axs[1].set_ylabel('Logl', fontsize=legend_font_size+1)\n",
    "axs[0].set_xlabel('Number of local splits', fontsize=legend_font_size+1)\n",
    "axs[1].set_xlabel('Number of local splits', fontsize=legend_font_size+1)\n",
    "plt.tight_layout()\n",
    "#if to_disk:\n",
    "#plt.savefig('figs/' + 'mimic_bal_trade-off.pdf')\n",
    "#else:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb8684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829d9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot legend; NOT FIXED\n",
    "fig, axs = plt.subplots(1,1, figsize=(8,10))\n",
    "    for i_method, params in enumerate(plot_group):\n",
    "        x = 1\n",
    "        y = 1\n",
    "        yerr = None    \n",
    "        if 'BCM' in params['name']:\n",
    "            axs.errorbar(None,None,\n",
    "                        yerr=yerr,\n",
    "                        marker='*', markersize=markersize,\n",
    "                        linewidth=0,\n",
    "                        linestyle = None,\n",
    "                        label=params['name'],\n",
    "                        color=params['colour'])\n",
    "        else:\n",
    "            if 'global' in params['name'] or 'trusted' in params['name']:\n",
    "                ls = '--'\n",
    "            else:\n",
    "                ls = '-'\n",
    "            axs.plot(x, y, label=params['name'],linestyle=ls, linewidth=linewidth, color=params['colour'])\n",
    "    axs.legend(loc=10, fontsize=legend_font_size, framealpha=1., mode='expand')\n",
    "    axs.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    if filename is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d432b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total samples with balanced mimic, 10 clients uniform split: 447\n",
    "print(1/config['sampling_frac_q'], np.floor(config['sampling_frac_q']*447) * (1/config['sampling_frac_q']))\n",
    "assert np.floor(config['sampling_frac_q'])*447 * (1/config['sampling_frac_q']) <= 447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6929d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( all_q, 1/all_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf950deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some funs for reading data\n",
    "\n",
    "def read_config(filename, failed_runs):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            apu = f.read()\n",
    "    except FileNotFoundError as err:\n",
    "        print(f\"Can't open file {filename}! Skipping\")\n",
    "        failed_runs.append(i_run)\n",
    "        return None\n",
    "    apu = jsonpickle.unpickler.decode(apu)\n",
    "    #print(apu)\n",
    "    return apu, failed_runs\n",
    "\n",
    "\n",
    "def read_results(filename, filename_bck):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            apu = f.read()\n",
    "        try:\n",
    "            apu = jsonpickle.unpickler.decode(apu)\n",
    "        except:\n",
    "            print(f'error in JSON decoding in run {filename}')\n",
    "            with open(filename, 'r') as f:\n",
    "                apu = f.read()\n",
    "            print('results from file: {}\\n{}'.format(filename,apu))\n",
    "            sys.exit()\n",
    "    except FileNotFoundError as err:\n",
    "        import json\n",
    "        with open(filename_bck, 'r') as f:\n",
    "            apu = f.read()\n",
    "            apu = json.loads(apu)\n",
    "        try:\n",
    "            #apu = jsonpickle.unpickler.decode(apu)\n",
    "            #print(apu)\n",
    "            apu = jsonpickle.decode(apu, keys=True)\n",
    "            #print(apu)\n",
    "            #print('at bck decode')\n",
    "        except:\n",
    "            print(f'error in JSON decoding in {filename_bck}')\n",
    "            #with open(filename, 'r') as f:\n",
    "            #    apu = f.read()\n",
    "            #print('results from file: {}\\n{}'.format(filename,apu))\n",
    "            sys.exit()\n",
    "    return apu\n",
    "\n",
    "def format_results(apu, run_id, client_measures, all_res):\n",
    "\n",
    "    all_res['client_train_res'][run_id] = {}\n",
    "\n",
    "    for k in client_measures:\n",
    "        all_res['client_train_res'][run_id][k] = np.zeros((\n",
    "            all_res['config'][run_id]['clients'],  \n",
    "            all_res['config'][run_id]['n_global_updates'],  \n",
    "            all_res['config'][run_id]['n_steps'],  \n",
    "            all_res['config'][run_id]['n_rng_seeds']\n",
    "            ))\n",
    "    measures = ['acc','logl']\n",
    "    posneg_measures = ['avg_prec_score','balanced_acc','f1_score']\n",
    "    \n",
    "    all_res['train_res'][run_id] = {}\n",
    "    all_res['validation_res'][run_id] = {}\n",
    "    for k in (measures+posneg_measures):\n",
    "        all_res['train_res'][run_id][k] = np.zeros((\n",
    "            all_res['config'][run_id]['n_global_updates'],  \n",
    "            all_res['config'][run_id]['n_rng_seeds']\n",
    "            ))\n",
    "        all_res['validation_res'][run_id][k] = np.zeros((\n",
    "            all_res['config'][run_id]['n_global_updates'],  \n",
    "            all_res['config'][run_id]['n_rng_seeds']\n",
    "            ))\n",
    "        all_res['train_res'][run_id]['best_'+k] = np.zeros((\n",
    "            all_res['config'][run_id]['n_rng_seeds']\n",
    "            ))\n",
    "        all_res['validation_res'][run_id]['best_'+k] = np.zeros((\n",
    "            all_res['config'][run_id]['n_rng_seeds']\n",
    "            ))\n",
    "\n",
    "    # does this work with sampling=seq?\n",
    "    if dataset_name != 'mnist':\n",
    "        # for plotting ROC curve for max logl global update, one for each seed\n",
    "        try:\n",
    "            all_res['validation_res'][run_id]['TPR'] = np.zeros((\n",
    "                all_res['config'][run_id]['n_rng_seeds'],\n",
    "                apu['validation_res_seed0']['posneg'][0]['n_points']\n",
    "                ))\n",
    "            all_res['validation_res'][run_id]['TNR'] = np.zeros((\n",
    "                all_res['config'][run_id]['n_rng_seeds'],\n",
    "                apu['validation_res_seed0']['posneg'][0]['n_points']\n",
    "                ))\n",
    "            all_res['validation_res'][run_id]['ROC_thresholds'] = np.linspace(0,1,apu['validation_res_seed0']['posneg'][0]['n_points'])\n",
    "        except:\n",
    "            print('error in posneg results')\n",
    "\n",
    "    for i_seed in range(all_res['config'][run_id]['n_rng_seeds']):\n",
    "        # logl, elbo, kl\n",
    "        '''\n",
    "        if i_seed == 0:\n",
    "            for k in apu[f'validation_res_seed{i_seed}']:\n",
    "                print(k)\n",
    "            print(apu[f'validation_res_seed{i_seed}']['acc'].shape )\n",
    "            print(apu[f'client_train_res_seed{i_seed}']['logl'].shape )\n",
    "        #'''\n",
    "        #sys.exit()\n",
    "        for k in client_measures:\n",
    "            try:\n",
    "                all_res['client_train_res'][run_id][k][:,:,:,i_seed] = apu[f'client_train_res_seed{i_seed}'][k]\n",
    "            except KeyError as err:\n",
    "                print(f'KeyError in run {i_run} (=folder)')\n",
    "                print(f'got\\n{apu}')\n",
    "                print(\"config: batch_size={}, jobid={}\".format(all_res['config'][run_id]['batch_size'], all_res['config'][run_id]['job_id'] ))\n",
    "                raise err\n",
    "\n",
    "        for k in measures:\n",
    "            all_res['train_res'][run_id][k][:,i_seed] = apu[f'train_res_seed{i_seed}'][k]\n",
    "            all_res['validation_res'][run_id][k][:,i_seed] = apu[f'validation_res_seed{i_seed}'][k]\n",
    "\n",
    "            all_res['train_res'][run_id]['best_'+k][i_seed] = np.amax(all_res['train_res'][run_id][k][:,i_seed])\n",
    "            all_res['validation_res'][run_id]['best_'+k][i_seed] = np.amax(all_res['validation_res'][run_id][k][:,i_seed])\n",
    "\n",
    "        # calculate true positive and true negative rates at global update with best logl\n",
    "        #print( all_res['validation_res'][run_id]['logl'][:,i_seed] )\n",
    "        best_global_logl = np.argmax( all_res['validation_res'][run_id]['logl'][:,i_seed] )\n",
    "        #print(all_res['validation_res'][run_id]['logl'][:,i_seed][best_global_logl])\n",
    "        #print(all_res['validation_res'][run_id]['best_logl'][i_seed])\n",
    "        # all_res['validation_res'][run_id] = {}\n",
    "        #sys.exit()\n",
    "\n",
    "        if dataset_name != 'mnist':\n",
    "            try:\n",
    "                all_res['validation_res'][run_id]['TPR'][i_seed,:] = apu[f\"validation_res_seed{i_seed}\"]['posneg'][best_global_logl]['TP']/( apu[f\"validation_res_seed{i_seed}\"]['posneg'][best_global_logl]['TP'] + apu[f\"validation_res_seed{i_seed}\"]['posneg'][best_global_logl]['FN'])\n",
    "\n",
    "                all_res['validation_res'][run_id]['TNR'][i_seed,:] = apu[f\"validation_res_seed{i_seed}\"]['posneg'][best_global_logl]['TN']/( apu[f\"validation_res_seed{i_seed}\"]['posneg'][best_global_logl]['TN'] + apu[f\"validation_res_seed{i_seed}\"]['posneg'][best_global_logl]['FP'])\n",
    "\n",
    "                # posneg = list of posneg dicts with len=n_global_updates\n",
    "                # NOTE: need to check format when n_seeds > 1\n",
    "                for i_global in range(all_res['config'][run_id]['n_global_updates']):\n",
    "                    for k in posneg_measures:\n",
    "                        #print(apu[f\"validation_res_seed0\"]['posneg'][i_global][k])\n",
    "                        all_res['train_res'][run_id][k][i_global,i_seed] = apu[f\"train_res_seed{i_seed}\"]['posneg'][i_global][k]\n",
    "                        all_res['validation_res'][run_id][k][i_global,i_seed] = apu[f\"validation_res_seed{i_seed}\"]['posneg'][i_global][k]\n",
    "                        #print(len(apu[f\"validation_res_seed0\"]['posneg']),len(apu[f\"train_res_seed0\"]['posneg'] ))\n",
    "                        #sys.exit()\n",
    "\n",
    "                for k in posneg_measures:\n",
    "                    all_res['train_res'][run_id]['best_'+k][i_seed] = np.amax(all_res['train_res'][run_id][k][:,i_seed])\n",
    "                    all_res['validation_res'][run_id]['best_'+k][i_seed] = np.amax(all_res['validation_res'][run_id][k][:,i_seed])\n",
    "\n",
    "            except:\n",
    "                print('error in AUCROC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
